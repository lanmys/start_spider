# Urllib库详解

##什么是Urllib

- python内置的http请求库：
  - urllib.request : 请求模块
  - urllin.error : 异常处理模块
  - urllib.parse : url解析模块
  - urllib.robotparser : robots.txt解析模块

- urllib.request
  - urlopen：发送request请求
    - urllib.request.urlopen(url,data=None,[timeout,]*,cafile=None,cadefault=False,context=None)
    ```python
    import urllib.request
    #默认传入get方法
    response = urllib.reuqest.urlopen('http://www.baidu.com')
    print(response.read().decode('utf-8'))   #调用read()方法读取，并使用decode()方法解码
    ```
- urllib.parse
    - 解析链接
    ```python
    import urllib.request
    import urllib.parse
    #传入data使用自动使用post请求
    data = bytes(urllib.parse.urlencode({'word':'hello'}),encoding='utf8')
    response = urllib.request.urlopen('http://www.baidu.com',data=data)
    print(response.read())
    ```

    - urlpase：urllib.parse.urlparse(urlstring,scheme='',allow_fragments=True)
      - urlstring : URL
      - scheme : 默认http协议
      - allow_fragments : 锚点
    ```python
    from urllib.parse import urlparse

    result = urlparse('www.baidu.com',scheme='https',allow_fragments=False)
    print(result)
    ```

    - urlunparse:拼接url
    ```python
    from urllib.parse import urlunparse
    data = ['http','www.baidu.com','index.html','user','a=6','comment']
    print(urlunparse(date))
    ```

    - urljoin：将两个url拼合，后面的url优先级高于前面的
    ```python
    from urllib.parse import urljoin

    print(urljoin('http://www.baidu.com','FAQ.html'))  #http://www.baidu.com/FAQ.html
    ```

    - urlencode : 把字典对象转换成get请求参数
    ```python
    from urllib.parse import urlencode

    params = {
        'name':'chen',
        'age':22
    }
    base_url = 'http://www.baidu.com?'
    url = base_url + urlencode(params)
    print(url)
    ```


- urllib.error
    - HTTPError处理异常：
这个类是专门处理http请求的异常，http请求会返回一个请求代码，因此HTTPError会有一个code属性。另外HTTP请求会有包含请求头信息，所以HTTPError还包含一个headers属性。HTTPError继承自URLError类，因此也包含有reason属性
    - 因为URLError是HTTPError的父类，所以在捕获异常的时候可以先找子类是否异常，如果子类找不到，再找父类即可
    ```python
    import socket
    import urllib.request
    import urllib.error

    try:
      response = urllib.request.urlopen('http://www.baidu.com',timeout=0.1) #设置请求超时时间为0.1
    except urllib.error.URLError as a:
      if isinstance(a.reason,socket.timeout):
          print('TIME OUT')
    ```

  ## 响应

    - 响应类型
      ```python
      import urllib.request

      response = urllib.request.urlopen('http://www.baidu.com')
      print(tyoe(response))   #<class 'http.clientHTTPResponse'>
      ```

      - 状态码、响应头
      ```python
      import urllib.request

      response = urllib.request.urlopen('http://www.baidu.com')
      print(response.status)   # 返回状态码
      print(response.getheaders())   #返回响应头
      print(response.getheaders('Server'))  #返回响应头中Server的属性
      ```

  - Request
    - 使用Request编辑生成一个Request对象,用来执行更复杂的操作
    ```python
    import urllib.reuqest

    URL = 'http://www.baidu.com'
    data = {
        'name':'chen'
    }
    headers = {
        'User-Agent':'...'
        'Host':'...'
    }
    request = urllib.request.Request(url=URL,data=data,headers=headers,method='post')
    #request.add_header("User-Agent":'...')   #也可以使用add_header()方法添加头
    response = urllib.request.urlopen(request)
    print(response.read().decode('utf-8'))
    ```

### Handler
  - IP代理:通过不断切换IP来防止被网站屏蔽
  ```python
  '''创建代理IP'''
  proxy_handler = urllib.request.ProxyHandler({
      'http':'http://127.0.0.1:9743'
      'https':'https://127.0.0.1:456'
  })
  opener = urllib.request.build_opener(proxy_handler)
  response = opener.open('http://www.baidu.com')
  print(response.read())
  ```
  - Cookie
    - 用于维持登录状态
  ```python
  import http.cookiejar,urllib.request
  #获取cookie的值
  cookie = http.cookiejar.CookieJar()  #生成cookie对象
  handler = urllib.request.HTTPCookieProcessor(cookie)
  opener = urllib.request.build_opener(handler)
  response = opener.open('http://www.baidu.com')
  for item in cookie:
      print(item.name+"="+item.value)
  ```

    - 在本地生成cookie.txt文件
  ```python
    import http.cookiejar,urllib.request

    finename = 'cookie.txt'
    cookie = http.cookiejar.MozillaCookieJar(filename)  #火狐浏览器的格式
    #cookie = http.cookiejar.LWPCookieJar(filename)   #LWP-Cookie-2.0格式
    handler = urllib.request.HTTPCookieProcessor(cookie)
    opener = urllib.request.build_opener(handler)
    response = opener.open('http://www.baidu.com')
    cookie.save(ignore_discard=True,ignore_expires=True)
  ```
    - 读取本地cookie文件
  ```python
  import http.cookiejar,urllib.request

  cookie = http.cookiejar.LWPCookieJar()
  cookie.load('cookie.txt',ignore_discard=True,ignore_expires=True)
  handler = urllib.request.HTTPCookieProcessor(cookie)
  opener = urllib.request.build_opener(handler)
  response = opener.open('http://www.baidu.com')
  ```
